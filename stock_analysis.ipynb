{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemiConductor Stock Market Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Yahoo Finance to collect stock information\n",
    "# Use LSTM to collect and predict future stock prices\n",
    "# Introduction of other factors to the stock analysis process\n",
    "\n",
    "# stock market prediction based on the performance of other stocks in the market in the same category\n",
    "\n",
    "# Focus will be AMD\n",
    "\n",
    "# depending on the perforamnce of other stocks such as Nvidia, Intel, etc. then the output should change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# yahoo Finance API library (useful?)\n",
    "import yfinance as yf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "# yf.pdr_override() # deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Spreads through 3 years of stock prices, prepare data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stock closing prices\n",
    "def fetch_stock(stock_symbol, start_years_ago=3):\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year - start_years_ago, end.month, end.day)\n",
    "    df = pdr.get_data_yahoo(stock_symbol, start=start, end=end)\n",
    "    # globals()[stock_symbol] = yf.download(stock_symbol, start=start, end=end)\n",
    "    # df = globals()[stock_symbol] = yf.download(stock_symbol, start=start, end=end)\n",
    "    return df.filter(['Adj Close']), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    return scaler, scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(scaled_data, window_size=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - window_size):\n",
    "        X.append(scaled_data[i:i+window_size, 0])\n",
    "        y.append(scaled_data[i+window_size, 0])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(scaled_data, training_data_len, window_size):\n",
    "    test_data = scaled_data[training_data_len - window_size:, :]\n",
    "    test_set = []\n",
    "    for i in range(window_size, len(test_data)):\n",
    "        test_set.append(test_data[i - window_size:i, 0])\n",
    "    # test_set = np.array(test_set)\n",
    "    # test_set = np.reshape(test_set, (test_set.shape[0], test_set.shape[1], 1))\n",
    "    return np.array(test_set).reshape(-1, window_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing original model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs the full LSTM model and transforms data of last 3 years, providing predicted values\n",
    "# if too lazy to pre setup\n",
    "def model_data(stock_symbol, epochs=100, batch_size=32, window_size=60):\n",
    "    # Fetch and normalize data\n",
    "    data, df = fetch_stock(stock_symbol)\n",
    "    dataset = data.values\n",
    "    training_data_len = int(np.ceil(len(dataset) * 0.95))\n",
    "    scaler, scaled_data = normalize_data(dataset)\n",
    "\n",
    "    # Create training and testing sets\n",
    "    X, y = split_data(scaled_data, window_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build and train the LSTM model\n",
    "    model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "\n",
    "    # Make predictions\n",
    "    test_set = prepare_test_data(scaled_data, training_data_len, window_size)\n",
    "    predictions = model.predict(test_set)\n",
    "    scaled_pred = scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Step 5: Prepare results\n",
    "    valid = data[training_data_len:]\n",
    "    valid['Predictions'] = scaled_pred\n",
    "\n",
    "    return df, valid, scaled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # outputs the results from model\n",
    "# stock_list = ['NVDA','TSM', 'INTC', 'AVGO', 'QCOM', 'AMD']\n",
    "\n",
    "# df_nvda_3, valid_nvda, pred_nvda = model_data(stock_list[0])\n",
    "\n",
    "# current_price = df_nvda_3['Adj Close'].iloc[-1]\n",
    "# threshold = 0.03\n",
    "\n",
    "# predicted_price = pred_nvda[-1]\n",
    "# if predicted_price > current_price * (1 + threshold):\n",
    "#     print(\"Consider buying the stock.\")\n",
    "# elif predicted_price < current_price * (1 - threshold):\n",
    "#     print(\"Consider selling the stock.\")\n",
    "# else:\n",
    "#     print(\"Hold the stock.\")\n",
    "\n",
    "# # takes around 1 minute to predict the next suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock prediction with other stocks\n",
    "Modify model to take predictions of last relevant stocks to influence price calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Nasdaq index as a macroeconomic indicator\n",
    "# add additional averages, RSI, or bollinger bands as features\n",
    "\n",
    "# stock prediction focuses semiconductor stocks\n",
    "\n",
    "# semiconductor stocks\n",
    "# nvidia, taiwan semiconductor manufacturing, intel, broadcom, qualcomm, AMD\n",
    "\n",
    "# 3 years\n",
    "stock_list = ['NVDA','TSM', 'INTC', 'AVGO', 'QCOM', 'AMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(stock_symbol, epochs=50, batch_size=16, window_size=60):\n",
    "    # Fetch data\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year - 5, end.month, end.day)\n",
    "    df = pdr.get_data_yahoo(stock_symbol, start=start, end=end)\n",
    "\n",
    "    # choosing only adj close prices\n",
    "    data = df.filter(['Adj Close'])\n",
    "    # convert to numpy array\n",
    "    dataset = data.values\n",
    "    # calculate the length of training data\n",
    "    training_data_len = int(np.ceil(len(dataset) * .95))\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "    # Prepare the feature (X) and target (y) sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - window_size):\n",
    "        X.append(scaled_data[i:i+window_size])\n",
    "        y.append(scaled_data[i+window_size])\n",
    "        \n",
    "    # Convert the lists into numpy arrays\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    test_data = scaled_data[training_data_len - window_size:, :]\n",
    "    test_set = []\n",
    "    for i in range(window_size, len(test_data)):\n",
    "        test_set.append(test_data[i-window_size:i, 0])\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_set = np.reshape(test_set, (test_set.shape[0], test_set.shape[1], 1))\n",
    "\n",
    "    prediction = model.predict(test_set)\n",
    "    scaled_pred = scaler.inverse_transform(prediction)\n",
    "\n",
    "    # Prepare results\n",
    "    valid = data[training_data_len:]\n",
    "    valid['Predictions'] = scaled_pred\n",
    "\n",
    "    return df, valid, scaled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stocks(stock_symbols, target_symbol='NVDA'):\n",
    "    # Fetch data for multiple stocks for 5 years\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year - 5, end.month, end.day)\n",
    "    # Fetch data for the list of stock symbols\n",
    "    data = pdr.get_data_yahoo(stock_symbols, start=start, end=end)\n",
    "    \n",
    "    # Select only the 'Close' prices for all stocks\n",
    "    stock_data = data['Close']\n",
    "\n",
    "    # Separate the target stock (NVDA) and other stocks\n",
    "    target_data = stock_data[target_symbol]  # The target stock data (NVDA)\n",
    "    other_stocks_data = stock_data.drop(columns=[target_symbol])  # All other stocks\n",
    "    \n",
    "    # Combine the data: We want the target stock's history based on all other stocks\n",
    "    dataset = pd.concat([other_stocks_data, target_data], axis=1).values\n",
    "\n",
    "    return dataset, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_symbol = 'NVDA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "# Fetch data for multiple stocks for 5 years\n",
    "end = datetime.now()\n",
    "start = datetime(end.year - 5, end.month, end.day)\n",
    "\n",
    "# Fetch data for multiple stocks for the last 5 years\n",
    "data = yf.download(stock_list, start=start, end=end)\n",
    "\n",
    "# data.head()\n",
    "\n",
    "stock_data = data['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target stock (NVDA) and other stocks\n",
    "target_data = stock_data[target_symbol]  # The target stock data (NVDA)\n",
    "other_stocks_data = stock_data.drop(columns=[target_symbol])  # All other stocks\n",
    "\n",
    "# Combine the data: We want the target stock's history based on all other stocks\n",
    "dataset = pd.concat([other_stocks_data, target_data], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, target_data = fetch_stocks(stock_list, 'NVDA')\n",
    "\n",
    "# Calculate the length for training data\n",
    "training_data_len = int(np.ceil(len(dataset) * .95))\n",
    "\n",
    "# Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60 # number of days to consider\n",
    "\n",
    "# Prepare the feature (X) and target (y) sequences\n",
    "X, y = [], []\n",
    "for i in range(len(scaled_data) - window_size):\n",
    "    X.append(scaled_data[i:i+window_size, :-1])  # All stock data for the window, excluding target stock\n",
    "    y.append(scaled_data[i+window_size, -1])  # Target stock's next day close price (last column)\n",
    "\n",
    "\n",
    "# Convert the lists into numpy arrays\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a7019bc080>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "test_data = scaled_data[training_data_len - window_size:, :]\n",
    "test_set = []\n",
    "for i in range(window_size, len(test_data)):\n",
    "    test_set.append(test_data[i - window_size:i, :-1])  # Use the history of other stocks\n",
    "\n",
    "test_set = np.array(test_set)\n",
    "test_set = np.reshape(test_set, (test_set.shape[0], test_set.shape[1], test_set.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367ms/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "prediction = model.predict(test_set)\n",
    "\n",
    "# Reshape the prediction to (num_samples, 1) for inverse transformation\n",
    "prediction = prediction.reshape(-1, 1)  # Flatten the output for inverse transformation\n",
    "\n",
    "# Inverse transform the predictions for the target stock (the last column)\n",
    "scaled_pred = scaler.inverse_transform(np.hstack([np.zeros((prediction.shape[0], scaled_data.shape[1] - 1)), prediction]))\n",
    "\n",
    "# Prepare results for the target stock\n",
    "valid = target_data[training_data_len:]\n",
    "valid = valid.to_frame()  # Convert to DataFrame if it's a Series\n",
    "valid['Predictions'] = scaled_pred[:, -1]  # Only take the last column for the target stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_multiple_stocks(stock_symbols, target_symbol='NVDA', epochs=100, batch_size=32, window_size=60):\n",
    "    # Fetch data for multiple stocks for 5 years\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year - 5, end.month, end.day)\n",
    "    \n",
    "    # Fetch data for the list of stock symbols\n",
    "    data = pdr.get_data_yahoo(stock_symbols, start=start, end=end)\n",
    "    \n",
    "    # Select only the 'Close' prices for all stocks\n",
    "    stock_data = data['Close']\n",
    "    \n",
    "    # Separate the target stock (NVDA) and other stocks\n",
    "    target_data = stock_data[target_symbol]  # The target stock data (NVDA)\n",
    "    other_stocks_data = stock_data.drop(columns=[target_symbol])  # All other stocks\n",
    "    \n",
    "    # Combine the data: We want the target stock's history based on all other stocks\n",
    "    dataset = pd.concat([other_stocks_data, target_data], axis=1).values\n",
    "    \n",
    "    # Calculate the length for training data\n",
    "    training_data_len = int(np.ceil(len(dataset) * .95))\n",
    "    \n",
    "    # Scale the data using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    \n",
    "    # Prepare the feature (X) and target (y) sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - window_size):\n",
    "        X.append(scaled_data[i:i+window_size, :-1])  # All stock data for the window, excluding target stock\n",
    "        y.append(scaled_data[i+window_size, -1])  # Target stock's next day close price (last column)\n",
    "\n",
    "    # Convert the lists into numpy arrays\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_data = scaled_data[training_data_len - window_size:, :]\n",
    "    test_set = []\n",
    "    for i in range(window_size, len(test_data)):\n",
    "        test_set.append(test_data[i - window_size:i, :-1])  # Use the history of other stocks\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_set = np.reshape(test_set, (test_set.shape[0], test_set.shape[1], test_set.shape[2]))\n",
    "\n",
    "    # Prediction\n",
    "    prediction = model.predict(test_set)\n",
    "\n",
    "    # Reshape the prediction to (num_samples, 1) for inverse transformation\n",
    "    prediction = prediction.reshape(-1, 1)  # Flatten the output for inverse transformation\n",
    "\n",
    "    # Inverse transform the predictions for the target stock (the last column)\n",
    "    scaled_pred = scaler.inverse_transform(np.hstack([np.zeros((prediction.shape[0], scaled_data.shape[1] - 1)), prediction]))\n",
    "\n",
    "    \n",
    "    # Prepare results for the target stock\n",
    "    valid = target_data[training_data_len:]\n",
    "    valid = valid.to_frame()  # Convert to DataFrame if it's a Series\n",
    "    valid['Predictions'] = scaled_pred[:, -1]  # Only take the last column for the target stock\n",
    "\n",
    "    return data, valid, scaled_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  6 of 6 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# data, valid, scaled_pred = preprocess_data_multiple_stocks(stock_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NVDA</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-05</th>\n",
       "      <td>107.209999</td>\n",
       "      <td>117.645996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-06</th>\n",
       "      <td>102.830002</td>\n",
       "      <td>116.374275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-09</th>\n",
       "      <td>106.470001</td>\n",
       "      <td>114.746329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>108.099998</td>\n",
       "      <td>114.042124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-11</th>\n",
       "      <td>116.910004</td>\n",
       "      <td>114.125062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  NVDA  Predictions\n",
       "Date                               \n",
       "2024-09-05  107.209999   117.645996\n",
       "2024-09-06  102.830002   116.374275\n",
       "2024-09-09  106.470001   114.746329\n",
       "2024-09-10  108.099998   114.042124\n",
       "2024-09-11  116.910004   114.125062"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "Makes predictions for the target stock in the past 5 years and use of other stocks to compare prices and adjust final prediction. R2 score is 0.78. This indicates that the model is fairly accurate in predicting the target stock's price. However, it's important to note that this model doesn't take into account other factors such as market trends, only comparing and measuring prices of other stocks to adjust its final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.7803438836871269\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(valid['NVDA'], valid['Predictions'])\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16116\\2751161046.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Define the threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.03\u001b[0m  \u001b[1;31m# 3% threshold for decision making\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Decision logic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mpredicted_price\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mcurrent_price\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Consider buying the stock.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mpredicted_price\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcurrent_price\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Consider selling the stock.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1578\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33mThe truth value of a \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m is ambiguous. \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Extract the last prediction as a scalar value\n",
    "predicted_price = scaled_pred[-1, 0]  # Ensure this is a scalar (single value)\n",
    "\n",
    "# Get the current price (last adjusted close price)\n",
    "current_price = data['Adj Close'].iloc[-1]  # Last adjusted close price\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 0.03  # 3% threshold for decision making\n",
    "\n",
    "# Decision logic\n",
    "if predicted_price > current_price * (1 + threshold):\n",
    "    print(\"Consider buying the stock.\")\n",
    "elif predicted_price < current_price * (1 - threshold):\n",
    "    print(\"Consider selling the stock.\")\n",
    "else:\n",
    "    print(\"Hold the stock.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
