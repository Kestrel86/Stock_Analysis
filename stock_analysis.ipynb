{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemiConductor Stock Market Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Yahoo Finance to collect stock information\n",
    "# Use LSTM to collect and predict future stock prices\n",
    "# Introduction of other factors to the stock analysis process\n",
    "\n",
    "# stock market prediction based on the performance of other stocks in the market in the same category\n",
    "\n",
    "# Focus will be AMD\n",
    "\n",
    "# depending on the perforamnce of other stocks such as Nvidia, Intel, etc. then the output should change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# yahoo Finance API library (useful?)\n",
    "import yfinance as yf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Spreads through 3 years of stock prices, prepare data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stock closing prices\n",
    "def fetch_stock(stock_symbol, start_years_ago=3):\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year - start_years_ago, end.month, end.day)\n",
    "    df = pdr.get_data_yahoo(stock_symbol, start=start, end=end)\n",
    "    # globals()[stock_symbol] = yf.download(stock_symbol, start=start, end=end)\n",
    "    # df = globals()[stock_symbol] = yf.download(stock_symbol, start=start, end=end)\n",
    "    return df.filter(['Adj Close']), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    return scaler, scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(scaled_data, window_size=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - window_size):\n",
    "        X.append(scaled_data[i:i+window_size, 0])\n",
    "        y.append(scaled_data[i+window_size, 0])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(scaled_data, training_data_len, window_size):\n",
    "    test_data = scaled_data[training_data_len - window_size:, :]\n",
    "    test_set = []\n",
    "    for i in range(window_size, len(test_data)):\n",
    "        test_set.append(test_data[i - window_size:i, 0])\n",
    "    # test_set = np.array(test_set)\n",
    "    # test_set = np.reshape(test_set, (test_set.shape[0], test_set.shape[1], 1))\n",
    "    return np.array(test_set).reshape(-1, window_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing original model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs the full LSTM model and transforms data of last 3 years, providing predicted values\n",
    "# if too lazy to pre setup\n",
    "def model_data(stock_symbol, epochs=100, batch_size=32, window_size=60):\n",
    "    # Fetch and normalize data\n",
    "    data, df = fetch_stock(stock_symbol)\n",
    "    dataset = data.values\n",
    "    training_data_len = int(np.ceil(len(dataset) * 0.95))\n",
    "    scaler, scaled_data = normalize_data(dataset)\n",
    "\n",
    "    # Create training and testing sets\n",
    "    X, y = split_data(scaled_data, window_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build and train the LSTM model\n",
    "    model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "\n",
    "    # Make predictions\n",
    "    test_set = prepare_test_data(scaled_data, training_data_len, window_size)\n",
    "    predictions = model.predict(test_set)\n",
    "    scaled_pred = scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Step 5: Prepare results\n",
    "    valid = data[training_data_len:]\n",
    "    valid['Predictions'] = scaled_pred\n",
    "\n",
    "    return df, valid, scaled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # outputs the results from model\n",
    "# stock_list = ['NVDA','TSM', 'INTC', 'AVGO', 'QCOM', 'AMD']\n",
    "\n",
    "# df_nvda_3, valid_nvda, pred_nvda = model_data(stock_list[0])\n",
    "\n",
    "# current_price = df_nvda_3['Adj Close'].iloc[-1]\n",
    "# threshold = 0.03\n",
    "\n",
    "# predicted_price = pred_nvda[-1]\n",
    "# if predicted_price > current_price * (1 + threshold):\n",
    "#     print(\"Consider buying the stock.\")\n",
    "# elif predicted_price < current_price * (1 - threshold):\n",
    "#     print(\"Consider selling the stock.\")\n",
    "# else:\n",
    "#     print(\"Hold the stock.\")\n",
    "\n",
    "# # takes around 1 minute to predict the next suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock prediction with other stocks\n",
    "Modify model to take predictions of last relevant stocks to influence price calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Nasdaq index as a macroeconomic indicator\n",
    "# add additional averages, RSI, or bollinger bands as features\n",
    "\n",
    "# stock prediction focuses semiconductor stocks\n",
    "\n",
    "# semiconductor stocks\n",
    "# nvidia, taiwan semiconductor manufacturing, intel, broadcom, qualcomm, AMD\n",
    "\n",
    "# 3 years\n",
    "stock_list = ['NVDA','TSM', 'INTC', 'AVGO', 'QCOM', 'AMD']\n",
    "\n",
    "# nvda_close, df_nvda = fetch_stock('NVDA')\n",
    "# tsm_close, df_tsm = fetch_stock('TSM')\n",
    "# intc_close, df_intc = fetch_stock('INTC')\n",
    "# avgo_close, df_avgo = fetch_stock('AVGO')\n",
    "# qcom_close, df_qcom = fetch_stock('QCOM')\n",
    "# amd_close, df_amd = fetch_stock('AMD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(stock_symbol, epochs=50, batch_size=16, window_size=60):\n",
    "    # Fetch data\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year - 5, end.month, end.day)\n",
    "    df = pdr.get_data_yahoo(stock_symbol, start=start, end=end)\n",
    "\n",
    "    # choosing only adj close prices\n",
    "    data = df.filter(['Adj Close'])\n",
    "    # convert to numpy array\n",
    "    dataset = data.values\n",
    "    # calculate the length of training data\n",
    "    training_data_len = int(np.ceil(len(dataset) * .95))\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "    # Prepare the feature (X) and target (y) sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - window_size):\n",
    "        X.append(scaled_data[i:i+window_size])\n",
    "        y.append(scaled_data[i+window_size])\n",
    "        \n",
    "    # Convert the lists into numpy arrays\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    test_data = scaled_data[training_data_len - window_size:, :]\n",
    "    test_set = []\n",
    "    for i in range(window_size, len(test_data)):\n",
    "        test_set.append(test_data[i-window_size:i, 0])\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_set = np.reshape(test_set, (test_set.shape[0], test_set.shape[1], 1))\n",
    "\n",
    "    prediction = model.predict(test_set)\n",
    "    scaled_pred = scaler.inverse_transform(prediction)\n",
    "\n",
    "    # Prepare results\n",
    "    valid = data[training_data_len:]\n",
    "    valid['Predictions'] = scaled_pred\n",
    "\n",
    "    return df, valid, scaled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_multiple_stocks(stock_symbols, epochs=100, batch_size=32, window_size=60):\n",
    "    # Fetch data for multiple stocks\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year - 5, end.month, end.day)\n",
    "    \n",
    "    # Fetch data for the list of stock symbols\n",
    "    data = pdr.get_data_yahoo(stock_symbols, start=start, end=end)\n",
    "    \n",
    "    # Select only the 'Close' prices for all stocks\n",
    "    stock_data = data['Close']\n",
    "    \n",
    "    # Convert the data into a numpy array (2D: [samples, features])\n",
    "    dataset = stock_data.values\n",
    "    \n",
    "    # Calculate the length for training data\n",
    "    training_data_len = int(np.ceil(len(dataset) * .95))\n",
    "    \n",
    "    # Scale the data using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    \n",
    "    # Prepare the feature (X) and target (y) sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - window_size):\n",
    "        X.append(scaled_data[i:i+window_size])  # All stock data for the window\n",
    "        y.append(scaled_data[i+window_size, 0])  # Target stock's next day close price (first column)\n",
    "\n",
    "    # Convert the lists into numpy arrays\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_data = scaled_data[training_data_len - window_size:, :]\n",
    "    test_set = []\n",
    "    for i in range(window_size, len(test_data)):\n",
    "        test_set.append(test_data[i - window_size:i, :])  # Including all stocks' data for the window\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_set = np.reshape(test_set, (test_set.shape[0], test_set.shape[1], test_set.shape[2]))\n",
    "\n",
    "    # Prediction\n",
    "    prediction = model.predict(test_set)\n",
    "\n",
    "    # Reshape the prediction to (num_samples, 1) for inverse transformation\n",
    "    prediction = prediction.reshape(-1, 1)  # Flatten the output for inverse transformation\n",
    "\n",
    "    # Inverse transform the predictions for the target stock (the first column)\n",
    "    scaled_pred = scaler.inverse_transform(np.hstack((prediction, np.zeros((prediction.shape[0], scaled_data.shape[1] - 1)))))\n",
    "    \n",
    "    # Prepare results for the target stock\n",
    "    valid = stock_data[training_data_len:]\n",
    "    valid['Predictions'] = scaled_pred[:, 0]  # Only take the first column for the target stock\n",
    "\n",
    "    return data, valid, scaled_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_multiple_stocks(stock_symbols, target_symbol='NVDA', epochs=100, batch_size=32, window_size=60):\n",
    "    # Fetch data for multiple stocks\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year - 5, end.month, end.day)\n",
    "    \n",
    "    # Fetch data for the list of stock symbols\n",
    "    data = pdr.get_data_yahoo(stock_symbols, start=start, end=end)\n",
    "    \n",
    "    # Select only the 'Close' prices for all stocks\n",
    "    stock_data = data['Close']\n",
    "    \n",
    "    # Separate the target stock (NVDA) and other stocks\n",
    "    target_data = stock_data[target_symbol]  # The target stock data (NVDA)\n",
    "    other_stocks_data = stock_data.drop(columns=[target_symbol])  # All other stocks\n",
    "    \n",
    "    # Combine the data: We want the target stock's history based on all other stocks\n",
    "    dataset = pd.concat([other_stocks_data, target_data], axis=1).values\n",
    "    \n",
    "    # Calculate the length for training data\n",
    "    training_data_len = int(np.ceil(len(dataset) * .95))\n",
    "    \n",
    "    # Scale the data using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    \n",
    "    # Prepare the feature (X) and target (y) sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - window_size):\n",
    "        X.append(scaled_data[i:i+window_size, :-1])  # All stock data for the window, excluding target stock\n",
    "        y.append(scaled_data[i+window_size, -1])  # Target stock's next day close price (last column)\n",
    "\n",
    "    # Convert the lists into numpy arrays\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_data = scaled_data[training_data_len - window_size:, :]\n",
    "    test_set = []\n",
    "    for i in range(window_size, len(test_data)):\n",
    "        test_set.append(test_data[i - window_size:i, :-1])  # Use the history of other stocks\n",
    "\n",
    "    test_set = np.array(test_set)\n",
    "    test_set = np.reshape(test_set, (test_set.shape[0], test_set.shape[1], test_set.shape[2]))\n",
    "\n",
    "    # Prediction\n",
    "    prediction = model.predict(test_set)\n",
    "\n",
    "    # Reshape the prediction to (num_samples, 1) for inverse transformation\n",
    "    prediction = prediction.reshape(-1, 1)  # Flatten the output for inverse transformation\n",
    "\n",
    "    # Inverse transform the predictions for the target stock (the last column)\n",
    "    scaled_pred = scaler.inverse_transform(np.hstack([np.zeros((prediction.shape[0], scaled_data.shape[1] - 1)), prediction]))\n",
    "\n",
    "    \n",
    "    # Prepare results for the target stock\n",
    "    valid = target_data[training_data_len:]\n",
    "    valid = valid.to_frame()  # Convert to DataFrame if it's a Series\n",
    "    valid['Predictions'] = scaled_pred[:, -1]  # Only take the last column for the target stock\n",
    "\n",
    "    return data, valid, scaled_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  6 of 6 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "data, valid, scaled_pred = preprocess_data_multiple_stocks(stock_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NVDA</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-04</th>\n",
       "      <td>106.209999</td>\n",
       "      <td>114.957789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-05</th>\n",
       "      <td>107.209999</td>\n",
       "      <td>113.475879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-06</th>\n",
       "      <td>102.830002</td>\n",
       "      <td>112.538263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-09</th>\n",
       "      <td>106.470001</td>\n",
       "      <td>111.109763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>108.099998</td>\n",
       "      <td>110.583215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  NVDA  Predictions\n",
       "Date                               \n",
       "2024-09-04  106.209999   114.957789\n",
       "2024-09-05  107.209999   113.475879\n",
       "2024-09-06  102.830002   112.538263\n",
       "2024-09-09  106.470001   111.109763\n",
       "2024-09-10  108.099998   110.583215"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.6831012109244186\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(valid['NVDA'], valid['Predictions'])\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.03\u001b[39m  \u001b[38;5;66;03m# 3% threshold for decision making\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Decision logic\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_price \u001b[38;5;241m>\u001b[39m current_price \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m threshold):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider buying the stock.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m predicted_price \u001b[38;5;241m<\u001b[39m current_price \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m threshold):\n",
      "File \u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1530\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Extract the last prediction as a scalar value\n",
    "predicted_price = scaled_pred[-1, 0]  # Ensure this is a scalar (single value)\n",
    "\n",
    "# Get the current price (last adjusted close price)\n",
    "current_price = data['Adj Close'].iloc[-1]  # Last adjusted close price\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 0.03  # 3% threshold for decision making\n",
    "\n",
    "# Decision logic\n",
    "if predicted_price > current_price * (1 + threshold):\n",
    "    print(\"Consider buying the stock.\")\n",
    "elif predicted_price < current_price * (1 - threshold):\n",
    "    print(\"Consider selling the stock.\")\n",
    "else:\n",
    "    print(\"Hold the stock.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
